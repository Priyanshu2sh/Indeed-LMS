{% extends "base.html" %} {% block content %}

<style>
  /* Disable scroll everywhere */
  html,
  body {
    height: 100%;
    margin: 0;
    padding: 0;
    overflow: hidden !important;
  }

  /* Disable base file buttons */
  .btn,
  button {
    pointer-events: none;
    opacity: 0.5;
  }

  /* Fullscreen countdown overlay */
  #countdown-overlay {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: rgba(0, 0, 0, 0.9);
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    color: #fff;
    font-size: 2rem;
    z-index: 9999;
  }

  /* Chat container */
  .chat-container {
    height: 80vh;
    display: flex;
    flex-direction: column;
    padding-bottom: 20vh;
  }

  /* Chat box (main area) */
  #chat-box {
    flex: 1;
    overflow-y: auto;
    border: 1px solid #ddd;
    border-radius: 10px;
    padding: 15px;
    background: #fff;
    position: relative; /* only for internal positioning */
  }

  .message {
    margin-bottom: 15px;
  }
  .bot {
    font-weight: bold;
    color: #0d6efd;
  }
  .user {
    font-weight: bold;
    color: #198754;
  }

  /* Camera fixed exactly at bottom-right inside chat-box */
  #camera {
    width: 200px;
    height: 150px;
    background: #000;
    border-radius: 10px;
    overflow: hidden;
    position: fixed;
    bottom: 20px;
    right: 20px;
    box-shadow: 0px 2px 10px rgba(0, 0, 0, 0.4);
    z-index: 1000;
  }

  #camera video {
    width: 100%;
    height: 100%;
    object-fit: cover;
  }

  .main-screen {
    text-align: center;
  }

  #mic-container {
    position: absolute;
    bottom: 7%;
    left: 50%;
    transform: translateX(-50%);
  }

  #mic-btn {
    background: #007bff;
    border: none;
    border-radius: 50%;
    padding: 15px;
    cursor: pointer;
    color: white;
    font-size: 20px;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.3);
  }

  #mic-btn:focus {
    outline: none;
  }

  #mic-icon {
    font-size: 22px;
  }

  .shape-left {
    left: 0;
    max-width: 0;
    overflow: hidden;
  }
</style>

<!-- Countdown Overlay -->
<div id="countdown-overlay">
  <div id="countdown-message">
    Your interview will start in <span id="countdown">5</span> sec
  </div>
</div>

<div class="main-screen container mt-4 chat-container">
  <!-- Heading -->
  <h3 class="mb-3">Practice Interview</h3>

  <!-- Chat Area -->
  <div id="chat-box">
  </div>
  <!-- Camera attached inside -->
  <div id="camera">
    <video id="video" autoplay muted></video>
  </div>

    <!-- Mic button bottom center -->
  <div id="mic-container">
    <button id="mic-btn">
      <i id="mic-icon" class="fas fa-microphone"></i>
    </button>
  </div>
</div>

<script>
  // === Camera & Mic Access ===
  navigator.mediaDevices
    .getUserMedia({ video: true, audio: true }) // ask for both
    .then((stream) => {
      // Attach camera stream to video element
      document.getElementById("video").srcObject = stream;

      // Save mic track but mute initially (control mic ON/OFF manually later)
      window.localStream = stream;
      window.audioTrack = stream.getAudioTracks()[0];
      if (window.audioTrack) {
        window.audioTrack.enabled = false; // keep mic OFF by default
        console.log("Mic access granted but OFF initially");
      }
    })
    .catch((err) => console.error("Camera/Mic access denied:", err));

  // === Countdown Overlay ===
  let countdown = 5;
  const countdownElement = document.getElementById("countdown");
  const overlay = document.getElementById("countdown-overlay");

  const timer = setInterval(() => {
    countdown--;
    countdownElement.textContent = countdown;
    if (countdown <= 0) {
      clearInterval(timer);
      overlay.style.display = "none"; // Hide overlay
      // TODO: Connect websocket here
      connectWebSocket();
    }
  }, 1000);
</script>

<script>
  let socket = null;
  let micIcon = document.getElementById("mic-icon");

  function connectWebSocket() {
    // Replace this with your actual backend WebSocket endpoint
    const wsUrl = "ws://127.0.0.1:8001/ws/interview/";

    socket = new WebSocket(wsUrl);

    socket.onopen = function () {
      console.log("âœ… WebSocket connected");
      // Example: send initial message if needed
      // socket.send(JSON.stringify({ message: "Interview started" }));
    };

    socket.onmessage = function (event) {
      const data = JSON.parse(event.data);
      console.log("ðŸ“© Message from server:", data);

      // Example: append message to chat box
      const chatBox = document.getElementById("chat-box");
      if (chatBox) {
        const msgDiv = document.createElement("div");
        msgDiv.className = "p-2 bg-light rounded mb-2";
        msgDiv.innerText = data.message || "[No message]";
        chatBox.appendChild(msgDiv);
        // Auto scroll to bottom
        chatBox.scrollTop = chatBox.scrollHeight;
      }

      // ðŸ”Š Play audio of the response text
      if (data.message) {
        micIcon.className = "fas fa-microphone-slash";
        const utterance = new SpeechSynthesisUtterance(data.message);
        utterance.lang = "en-US"; // you can change voice language
        utterance.rate = 1; // speed (0.5 - 2)
        utterance.pitch = 1; // pitch (0 - 2)
        window.speechSynthesis.speak(utterance);

        utterance.onend = () => {
          startMic();
        };
      }
    };

    socket.onclose = function () {};

    socket.onerror = function (error) {};
  }

  let micStream;
  let mediaRecorder;
  let audioChunks = [];
  let audioContext, analyser, dataArray, source;
  let silenceTimer = null;

  async function startMic() {
    micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
    mediaRecorder = new MediaRecorder(micStream, { mimeType: "audio/webm" });

    // Collect chunks only while recording
    mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.push(event.data);
      }
    };

    // When speech fully ends â†’ send blob
    mediaRecorder.onstop = () => {
      if (audioChunks.length > 0) {
        const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
        audioChunks = []; // reset for next speech

        if (socket && socket.readyState === WebSocket.OPEN) {
          console.log("ðŸ“¤ Sending full speech to backend:", audioBlob);
          socket.send(audioBlob);
        }
      }

      stopMic(); // fully stop mic after sending
    };

    // ---- Setup analyser for speech detection ----
    audioContext = new AudioContext();
    source = audioContext.createMediaStreamSource(micStream);
    analyser = audioContext.createAnalyser();
    analyser.fftSize = 2048;
    dataArray = new Uint8Array(analyser.fftSize);
    freqData = new Uint8Array(analyser.frequencyBinCount); // frequency-domain
    source.connect(analyser);

    detectSpeech(); // start detection loop

    document.getElementById("mic-icon").className =
      "fas fa-microphone text-green-500";
  }

  function detectSpeech() {
    analyser.getByteTimeDomainData(dataArray);
    // --- Frequency check (human voice range ~85 Hz to 4000 Hz) ---
    analyser.getByteFrequencyData(freqData);

    const sampleRate = audioContext.sampleRate; // usually 44100 Hz
    const nyquist = sampleRate / 2; // max frequency
    const freqStep = nyquist / freqData.length;

    let humanEnergy = 0;
    let totalEnergy = 0;

    for (let i = 0; i < freqData.length; i++) {
      const freq = i * freqStep;
      const mag = freqData[i];
      totalEnergy += mag;

      if (freq >= 85 && freq <= 4000) {
        humanEnergy += mag;
      }
    }

    const humanRatio = humanEnergy / (totalEnergy + 1);

    // Calculate RMS volume
    let sumSquares = 0;
    for (let i = 0; i < dataArray.length; i++) {
      const val = (dataArray[i] - 128) / 128;
      sumSquares += val * val;
    }
    const rms = Math.sqrt(sumSquares / dataArray.length);

    const speaking = rms > 0.02 && humanRatio > 0.3;

    if (speaking) {
      console.log("ðŸŽ¤ Speaking...");
      if (mediaRecorder.state === "inactive") {
        console.log("â–¶ï¸ Started recording...");
        mediaRecorder.start();
      }

      // Reset silence timer (user resumed speaking)
      if (silenceTimer) {
        clearTimeout(silenceTimer);
        silenceTimer = null;
      }
    } else {
      // If recording â†’ consider stopping only after a long pause
      if (mediaRecorder.state === "recording" && !silenceTimer) {
        silenceTimer = setTimeout(() => {
          console.log("â¹ï¸ Long silence detected â†’ stopping mic");
          mediaRecorder.stop();
        }, 5000); // 5 sec pause considered end of speech
      }
    }

    requestAnimationFrame(detectSpeech);
  }

  // Stop mic completely
  function stopMic() {
    if (micStream) {
      micStream.getTracks().forEach((track) => track.stop());
      micStream = null;
    }
    if (audioContext) {
      audioContext.close();
      audioContext = null;
    }
    document.getElementById("mic-icon").className =
      "fas fa-microphone text-red-500";
  }

  // Stop Mic
  function stopMic() {
    if (mediaRecorder && mediaRecorder.state !== "inactive") {
      mediaRecorder.stop();
    }
    if (micStream) {
      micStream.getTracks().forEach((track) => track.stop());
    }
    document.getElementById("mic-icon").className =
      "fas fa-microphone-slash text-red-500";
    micEnabled = false;
  }
</script>

{% endblock content %}
